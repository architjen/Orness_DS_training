{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"mount_file_id":"1_tAHeIIkvBgZn_5jJRx0F_N_7Lj58Pa4","authorship_tag":"ABX9TyNej6BcCqiVS/Q6VQCuS4iQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Data Acquisition and Understanding"],"metadata":{"id":"LuxxphyYD2xa"}},{"cell_type":"markdown","source":["Once the APIs are built and the raw data is available its the job of data engineer to perform the data acquisition and to provide the data further to data scientist for further exploitation tasks. "],"metadata":{"id":"0alyGpjzgKju"}},{"cell_type":"markdown","source":["Data engineers basically work on principles of ETL, i.e., ```Extract Transform and Load```. Although it could be ELT as well. The order is not really important for us now (at this level)"],"metadata":{"id":"DbwRGNE6wWZ4"}},{"cell_type":"markdown","source":["Difference between ETL and ELT can be found [here](https://www.oracle.com/fr/database/elt-vs-etl.html)"],"metadata":{"id":"IypWDOnn2EWF"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"N5x-n5jXDpao"},"outputs":[],"source":["# Lets start by importing the libraries \n","import os\n","import glob\n","import pandas as pd # library for data wrangling"]},{"cell_type":"markdown","source":["We'll do here is to ```Extract``` the data coming from different sources (csv and json) of file"],"metadata":{"id":"TjpIQF0-uOin"}},{"cell_type":"code","source":["# the path to our data files\n","path = \"/content/drive/MyDrive/Colab_Notebooks/orness/acquisition_data/\""],"metadata":{"id":"mxmuBgmK5Js-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Listing all the csv files\n","for csvfile in glob.glob(path+\"*.csv\"):\n","  print(csvfile)"],"metadata":{"id":"JpBM-3Cx58ef"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# listing json file\n","for jsonfile in glob.glob(path+\"*.json\"):\n","  print(jsonfile)"],"metadata":{"id":"ptN1vO6Tioh0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Looping throught the files, turning them into a DataFrame and appending it to a list\n","df_list = []\n","for csvfile in glob.glob(path+\"*.csv\"):\n","  df_tmp = pd.read_csv(csvfile)\n","  df_list.append(df_tmp)\n","  \n","# Finally concatinating all the dataframes   \n","df_csv = pd.concat(df_list)"],"metadata":{"id":"3h8x0PJUkNpF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---------Looking at the pandas DataFrame for a brief moment -------\n","\n","pd.DataFrame.iloc  \n","pd.DataFrame['col']  \n","pd.DataFrame[index]  \n","pd.DataFrame.shape  \n","pd.DataFrame.dtypes  \n","\n","Follow the rich doc of pandas [here](https://pandas.pydata.org/docs/user_guide/index.html)"],"metadata":{"id":"ZkOTwIxnV7cI"}},{"cell_type":"code","source":["df_csv.head(10)"],"metadata":{"id":"GLcruSSaPwsi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_json = pd.read_json(path+'winequality_4.json', orient ='records', lines=True)"],"metadata":{"id":"jZys4cQhg2jT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_csv.dtypes"],"metadata":{"id":"B7JD3ro58_fI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_json.dtypes"],"metadata":{"id":"MLcsSpuJfyJW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data Cleaning"],"metadata":{"id":"YWN9hkja5Mv9"}},{"cell_type":"markdown","source":["Why cleaning the dataframes seems to be a such a crutial part?  \n","\n","Python is (luckily) not a statically typed language. That is why we can develop so fast. The drawback of a dynamically typed language is that we may run into more problems during runtime than we would using a strict static typing scheme. Python will at least throw an error during runtime"],"metadata":{"id":"6VCQ46vuUS7E"}},{"cell_type":"markdown","source":["Hence we need to typecast the data before the processing or analysis.  "],"metadata":{"id":"ypfgK_gD7UTt"}},{"cell_type":"markdown","source":["Advance data cleaning technique could involve: Data Validation (using pydantic and pandas), creating generators list (for infinite sequences of dataflow)"],"metadata":{"id":"aZrhu2p18Jqd"}},{"cell_type":"markdown","source":["```Transform``` into the same data type"],"metadata":{"id":"2fYhkmX3T1In"}},{"cell_type":"code","source":["# trying to convert each entry to float \n","df_csv['alcohol'] = df_csv['alcohol'].astype(float)"],"metadata":{"id":"HNCPJx4w9vUN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# printing the str values \n","for i in df_csv['alcohol']:\n","  try:\n","    float(i)\n","  except:\n","    print(i)"],"metadata":{"id":"vIm7h_htCiNw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_csv['alcohol'] = df_csv['alcohol'].str.replace('%', '')\n","df_csv['alcohol'] = df_csv['alcohol'].astype(float)"],"metadata":{"id":"RhuJTx1AoOCl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# checking the null values in the dataframe\n","df_csv.isnull().sum()"],"metadata":{"id":"G4WQ7qZlB2b0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_csv.info()"],"metadata":{"id":"7CfmA4OcNx8o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For the other json file we will parse it using the pydantic library.  \n","[Pydantic](https://docs.pydantic.dev/usage/validators/) enforces type hints at runtime, and provides user friendly errors when data is invalid.  \n","\n"],"metadata":{"id":"_FZlyAhSqkLb"}},{"cell_type":"code","source":["from pydantic import ValidationError, Field, BaseModel"],"metadata":{"id":"w_Cha6v7UH8L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DataValidation(BaseModel):\n","  fixed_acidity: float\n","  volatile_acidity: float\n","  citric_acid: float\n","  residual_sugar: float\n","  chlorides: float\n","  free_sulfur_dioxide: float\n","  total_sulfur_dioxide: float\n","  density: float\n","  pH: float #= Field(ge=6, le=8, description=\"ph of water\")\n","  sulphates: float\n","  alcohol: float\n","  TARGET: int"],"metadata":{"id":"1HQ6paZVU-zf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dict_to_val = df_json.to_dict(orient=\"records\")"],"metadata":{"id":"hzpVvs_Tnlw1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in dict_to_val:\n","  print(i)"],"metadata":{"id":"rAGHUqVBj8JS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["error_count = 0\n","verified_data = []\n","for i in dict_to_val:\n","  try:    \n","    verified_data.append(DataValidation(**i))\n","  except ValidationError as ve:\n","    print(f\"row: {i}, error: {ve}\")\n","    error_count += 1"],"metadata":{"id":"TEwBXtVBpRpI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["error_count"],"metadata":{"id":"yx78G70Egvf_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_json_clean = pd.DataFrame([s.__dict__ for s in verified_data])"],"metadata":{"id":"vle69CquDjAW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_json_clean.dtypes"],"metadata":{"id":"3msqq9tDEmv2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["```Load``` the data to have a final data"],"metadata":{"id":"VVa1rJE0pCzc"}},{"cell_type":"code","source":["df_clean = pd.concat([df_json_clean, df_csv], ignore_index = True)"],"metadata":{"id":"iJCUBpvnFjS3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["![ETL](https://drive.google.com/uc?id=19vKLoFRkSCkH7yfG4KZBs71L8jyQK0y2)"],"metadata":{"id":"KHuMzCQ6c02h"}},{"cell_type":"code","source":[],"metadata":{"id":"YlQH-i90dFmv"},"execution_count":null,"outputs":[]}]}